{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "tRENk7XqslPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForSeq2SeqLM\n",
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "import spacy\n",
        "import torch\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "EChZN67jZZ_u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "nmyF-17Uck0j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\")"
      ],
      "metadata": {
        "id": "NFKyOHAivK3U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def compute_coherence(text, window_size=2):\n",
        "    sentences = text.split(\". \")\n",
        "    if len(sentences) < 2:\n",
        "        return 1\n",
        "\n",
        "    embeddings = model.encode(sentences, convert_to_numpy=True)\n",
        "\n",
        "    similarities = []\n",
        "    for i in range(len(sentences) - window_size + 1):\n",
        "        window_embs = embeddings[i : i + window_size]\n",
        "        for j in range(len(window_embs) - 1):\n",
        "            sim = 1 - cosine(window_embs[j], window_embs[j + 1])\n",
        "            similarities.append(sim)\n",
        "\n",
        "    avg_similarity = np.mean(similarities) if similarities else 0.0\n",
        "    return float(avg_similarity)"
      ],
      "metadata": {
        "id": "XFP2AI3_8Nmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea604f7-e211-4a0f-fbcb-a571fafb1570"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "def analyze_text_complexity(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    total_words = len([token.text for token in doc if token.is_alpha])\n",
        "    total_sentences = len(list(doc.sents))\n",
        "    total_chars = sum(len(token.text) for token in doc)\n",
        "    unique_words = len(set(token.text.lower() for token in doc if token.is_alpha))\n",
        "\n",
        "    content_words_noun = [token for token in doc if token.pos_ in [\"NOUN\"]]\n",
        "    content_words_verb = [token for token in doc if token.pos_ in [\"VERB\"]]\n",
        "    content_words_adv = [token for token in doc if token.pos_ in [\"ADV\"]]\n",
        "    content_words_adj = [token for token in doc if token.pos_ in [\"ADJ\"]]\n",
        "\n",
        "    lexical_density_noun = round(len(content_words_noun) / total_words,4) if total_words > 0 else 0\n",
        "    lexical_density_verb = round(len(content_words_verb) / total_words,4) if total_words > 0 else 0\n",
        "    lexical_density_adv = round(len(content_words_adv) / total_words,4) if total_words > 0 else 0\n",
        "    lexical_density_adj = round(len(content_words_adj) / total_words,4) if total_words > 0 else 0\n",
        "\n",
        "    ttr = unique_words / total_words if total_words > 0 else 0\n",
        "    avg_word_length = total_chars / total_words if total_words > 0 else 0\n",
        "    avg_sentence_length = total_words / total_sentences if total_sentences > 0 else 0\n",
        "\n",
        "    max_depth = max([token.i - token.head.i for token in doc if token.head != token]) if doc else 0\n",
        "\n",
        "    clause_count = sum(1 for token in doc if token.dep_ in {\"conj\", \"ccomp\", \"advcl\"})\n",
        "\n",
        "    named_entities = len(doc.ents)\n",
        "\n",
        "    coherence = compute_coherence(text,window_size=2)\n",
        "\n",
        "    return {\n",
        "        \"Lexical Density Noun\": round(lexical_density_noun,4),\n",
        "        \"Lexical Density Verb\": round(lexical_density_verb,4),\n",
        "        \"Lexical Density Adverb\": round(lexical_density_adv,4),\n",
        "        \"Lexical Density Adjective\": round(lexical_density_adj,4),\n",
        "        \"Type-Token Ratio\": round(ttr,4),\n",
        "        \"Average Word Length\": round(avg_word_length,4),\n",
        "        \"Average Sentence Length\": round(avg_sentence_length,4),\n",
        "        \"Max Syntactic Depth\": max_depth,\n",
        "        \"Clauses per Sentence\": round(clause_count / total_sentences,4) if total_sentences > 0 else 0,\n",
        "        \"Named Entity Count\": named_entities,\n",
        "        'Coherence':round(coherence,4)}\n"
      ],
      "metadata": {
        "id": "UpfAQrB9wT1I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "SqGG4OnnAf_E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution"
      ],
      "metadata": {
        "id": "5vCIT-S5Covo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "rag_dataset = load_dataset(\"neural-bridge/rag-dataset-1200\")\n"
      ],
      "metadata": {
        "id": "ccVc_WUbcgOF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "GwRgoD6Pdifa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if token.is_alpha and not token.is_stop and token.pos_ in ['NOUN','VERB','ADJ','ADV']]\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    return set(ent.text for ent in doc.ents)\n",
        "\n",
        "def compute_token_overlap(context, answer):\n",
        "    context_tokens = set(tokenize_text(context))\n",
        "    answer_tokens = set(tokenize_text(answer))\n",
        "\n",
        "    context_answer_overlap = len(context_tokens.intersection(answer_tokens)) / len(answer_tokens) if len(answer_tokens) > 0 else 0\n",
        "\n",
        "    return context_answer_overlap\n",
        "\n",
        "def improved_factual_matching(context, answer):\n",
        "\n",
        "    token_overlap_score = compute_token_overlap(context, answer)\n",
        "\n",
        "\n",
        "    context_entities = extract_entities(context)\n",
        "\n",
        "    answer_entities = extract_entities(answer)\n",
        "    entity_overlap = len(context_entities.intersection(answer_entities)) / len(answer_entities) if len(answer_entities) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'token_overlap_score': token_overlap_score,\n",
        "        'entity_overlap': entity_overlap,\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "context = rag_dataset['train'][125]['context']\n",
        "question = rag_dataset['train'][125]['question']\n",
        "answer = rag_dataset['train'][125]['answer']\n",
        "\n",
        "matching_results = improved_factual_matching(context, answer)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9oB2X1hBVfC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matching_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z2grf5JJVE9",
        "outputId": "542a51e8-aa1e-4f10-c0f1-a1b1b68ac69d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'token_overlap_score': 1.0, 'entity_overlap': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_text_complexity(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCEq5U8LIF43",
        "outputId": "bed4be78-d3a6-4782-cc04-109786fef172"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Lexical Density Noun': 0.2371,\n",
              " 'Lexical Density Verb': 0.1267,\n",
              " 'Lexical Density Adverb': 0.0436,\n",
              " 'Lexical Density Adjective': 0.0599,\n",
              " 'Type-Token Ratio': 0.4823,\n",
              " 'Average Word Length': 4.9605,\n",
              " 'Average Sentence Length': 17.0698,\n",
              " 'Max Syntactic Depth': 31,\n",
              " 'Clauses per Sentence': 1.093,\n",
              " 'Named Entity Count': 60,\n",
              " 'Coherence': 0.3815}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_text_complexity(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFNcmjU7IwFL",
        "outputId": "8d465b70-a264-46c0-ef7a-f31ce5692b6b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Lexical Density Noun': 0.2,\n",
              " 'Lexical Density Verb': 0.08,\n",
              " 'Lexical Density Adverb': 0.04,\n",
              " 'Lexical Density Adjective': 0.0,\n",
              " 'Type-Token Ratio': 0.88,\n",
              " 'Average Word Length': 5.04,\n",
              " 'Average Sentence Length': 25.0,\n",
              " 'Max Syntactic Depth': 20,\n",
              " 'Clauses per Sentence': 1.0,\n",
              " 'Named Entity Count': 4,\n",
              " 'Coherence': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context"
      ],
      "metadata": {
        "id": "dAJJ3lF0JwvM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "d6136275-01f9-4418-ac75-68467e413558"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a classic and therefore enjoy without a second thought. However, a closer look into these rock hits reveals the hidden meaning behind why the lyrics were written and what hidden messages the artists are trying to communicate.\\nHere are five influential pieces that take a little digging to fully understand the hidden messages in the songs:\\n“Total Eclipse of the Heart” – Bonnie Tyler\\nUpon first listen, “Total Eclipse of the Heart” comes off as an emotionally charged love song. What most listeners don’t know is that it was originally written as a vampire love song and was featured in Jim Steinman’s Broadway musical, Dance of the Vampires.\\nWhen you listen closely, lyrics like “Once upon a time there was light in my life, but now there’s only love in the dark” lend themselves to a more cryptic theme. Although the song never outright mentions vampires or coffins, the tones of the song make sense for use in the musical. Despite its roots, this rock music meaning is widely known as a love song (for mortals).\\n“London Calling” – The Clash\\nOften interpreted as a politically charged anthem about the British government, “London Calling” was actually inspired by Joe Strummer’s fear of drowning. At the time of the song’s creation, headlines warning Londoners about the flooding of the Thames river sparked fear – and inspiration – in the band.\\nLyrics like “The ice age is coming, the sun is zooming in. Meltdown expected, the wheat is growin’ thin. Engines stop running, but I have no fear ’cause London is drowning, and I, I live by the river” portray the warning as the start of the end of the world. Mick Jones explains that while the song was inspired by Strummer’s fear of drowning, it unfolded into a general warning of doom.\\n“Hey Jude” – The Beatles\\nThis Beatles hit was written by Paul McCartney shortly after John Lennon’s divorce from his wife, Cynthia. Lennon’s son, Julian, who was five at the time, was having a rough time accepting his parent’s divorce. McCartney was close to the family and developed a special relationship with Julian, who he empathized with. He began singing “Hey Jules…” to himself and wrote the original draft of “Hey Jude” as a gesture of comfort for Julian.\\nHe later expanded the lyrics and changed the name in the song to “Jude” to fit his style. Lines like “And anytime you feel the pain, hey Jude, refrain, don’t carry the world upon your shoulders” depict the comforting tone that McCartney was trying to communicate. It wasn’t until 20 years later that Julian discovered the song was originally created for him.\\n“American Pie” – Don McLean\\nWhile the upbeat pace and catchy lines of this song sound like a cheerful anthem of American youth, it was actually written as a goodbye to an era. In 1959, Buddy Holly, along with several other musical superstars, boarded a plane that tragically crashed, killing everyone aboard. This rocked the music world and inspired Don McLean to write “American Pie” in 1972.\\nLyrics like “The day the music died” are in reference to that fatal plane crash, which symbolizes the loss of the romance and idyll that characterized the fifties and sixties. In short, the song illustrates the departure of the American dream in a way that is simultaneously light and comforting.\\n“Hotel California” – The Eagles\\nThis mellow, catchy hit seems like a west-coast-vacation-inspired ballad, but upon closer inspection, the lyrics are actually calling out excess in America. The song depicts the high life in LA as interpreted by Glenn Frey, Don Henley, and Don Felder. Lyrics like “They stab it with their steely knives, but they just can’t kill the beast” and “You can check out any time you like, but you can never leave!” expose the darker side of an extravagant LA lifestyle. In the 2013 History of the Eagles documentary, Don Henley explained, “It’s a song about a journey from innocence to experience.”\\nWhether it’s a political statement decorated with a cheerful chorus or a personal message of comfort embedded in a chart-topping classic, hidden messages in songs add depth to the artist’s work. Often times, song inspiration sparks from societal events, personal struggles and triumphs, cultural nostalgia, or opinions of the artist. Taking the time to pick apart the lyrics and identify the significance behind them can cultivate a deeper connection to the song and, as a result, the artist.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "id": "UIywy_RKueSx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61981df2-6566-4f07-c6ce-45cda1b8d86c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Total Eclipse of the Heart\" was originally written as a vampire love song and was featured in Jim Steinman’s Broadway musical, Dance of the Vampires.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kSNpFE2RJ7Qs"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}